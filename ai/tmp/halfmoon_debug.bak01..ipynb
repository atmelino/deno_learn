{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pprint\n",
    "from mycrograd_debug.engine_debug import Value\n",
    "from mycrograd_debug.nn_debug import Neuron, Layer, MLP\n",
    "from mycrograd_debug.drawviz_debug import draw_dot,draw_nn\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module nn MLP: structure [3, 4, 4, 1]\n",
      "[   Value(name=v001,layernumber=L1,neuronnumber=N1,weightnumber=,type=w1,data=0.23550571390294128, grad=0),\n",
      "    Value(name=v002,layernumber=L1,neuronnumber=N1,weightnumber=,type=w2,data=0.06653114721000164, grad=0),\n",
      "    Value(name=v003,layernumber=L1,neuronnumber=N1,weightnumber=,type=w3,data=-0.26830328150124894, grad=0),\n",
      "    Value(name=v004,layernumber=L1,neuronnumber=N1,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v005,layernumber=L1,neuronnumber=N2,weightnumber=,type=w1,data=0.1715747078045431, grad=0),\n",
      "    Value(name=v006,layernumber=L1,neuronnumber=N2,weightnumber=,type=w2,data=-0.6686254326224383, grad=0),\n",
      "    Value(name=v007,layernumber=L1,neuronnumber=N2,weightnumber=,type=w3,data=0.6487474938152629, grad=0),\n",
      "    Value(name=v008,layernumber=L1,neuronnumber=N2,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v009,layernumber=L1,neuronnumber=N3,weightnumber=,type=w1,data=-0.23259038277158273, grad=0),\n",
      "    Value(name=v010,layernumber=L1,neuronnumber=N3,weightnumber=,type=w2,data=0.5792256498313748, grad=0),\n",
      "    Value(name=v011,layernumber=L1,neuronnumber=N3,weightnumber=,type=w3,data=0.8434530197925192, grad=0),\n",
      "    Value(name=v012,layernumber=L1,neuronnumber=N3,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v013,layernumber=L1,neuronnumber=N4,weightnumber=,type=w1,data=-0.3847332240409951, grad=0),\n",
      "    Value(name=v014,layernumber=L1,neuronnumber=N4,weightnumber=,type=w2,data=0.9844941451716409, grad=0),\n",
      "    Value(name=v015,layernumber=L1,neuronnumber=N4,weightnumber=,type=w3,data=-0.5901079958448365, grad=0),\n",
      "    Value(name=v016,layernumber=L1,neuronnumber=N4,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v017,layernumber=L2,neuronnumber=N1,weightnumber=,type=w1,data=0.31255526637777775, grad=0),\n",
      "    Value(name=v018,layernumber=L2,neuronnumber=N1,weightnumber=,type=w2,data=0.8246106857787521, grad=0),\n",
      "    Value(name=v019,layernumber=L2,neuronnumber=N1,weightnumber=,type=w3,data=-0.7814232047574572, grad=0),\n",
      "    Value(name=v020,layernumber=L2,neuronnumber=N1,weightnumber=,type=w4,data=0.6408752595662697, grad=0),\n",
      "    Value(name=v021,layernumber=L2,neuronnumber=N1,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v022,layernumber=L2,neuronnumber=N2,weightnumber=,type=w1,data=-0.20252189189007108, grad=0),\n",
      "    Value(name=v023,layernumber=L2,neuronnumber=N2,weightnumber=,type=w2,data=-0.8693137391598071, grad=0),\n",
      "    Value(name=v024,layernumber=L2,neuronnumber=N2,weightnumber=,type=w3,data=0.39841666323128555, grad=0),\n",
      "    Value(name=v025,layernumber=L2,neuronnumber=N2,weightnumber=,type=w4,data=-0.3037961142013801, grad=0),\n",
      "    Value(name=v026,layernumber=L2,neuronnumber=N2,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v027,layernumber=L2,neuronnumber=N3,weightnumber=,type=w1,data=-0.19282493884310759, grad=0),\n",
      "    Value(name=v028,layernumber=L2,neuronnumber=N3,weightnumber=,type=w2,data=0.6032250931493106, grad=0),\n",
      "    Value(name=v029,layernumber=L2,neuronnumber=N3,weightnumber=,type=w3,data=0.6001302646227185, grad=0),\n",
      "    Value(name=v030,layernumber=L2,neuronnumber=N3,weightnumber=,type=w4,data=0.32749776568749045, grad=0),\n",
      "    Value(name=v031,layernumber=L2,neuronnumber=N3,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v032,layernumber=L2,neuronnumber=N4,weightnumber=,type=w1,data=0.6650130652363544, grad=0),\n",
      "    Value(name=v033,layernumber=L2,neuronnumber=N4,weightnumber=,type=w2,data=0.1889136153241595, grad=0),\n",
      "    Value(name=v034,layernumber=L2,neuronnumber=N4,weightnumber=,type=w3,data=-0.07813264062433589, grad=0),\n",
      "    Value(name=v035,layernumber=L2,neuronnumber=N4,weightnumber=,type=w4,data=0.9151267732861252, grad=0),\n",
      "    Value(name=v036,layernumber=L2,neuronnumber=N4,weightnumber=,type=b,data=0, grad=0),\n",
      "    Value(name=v037,layernumber=L3,neuronnumber=N1,weightnumber=,type=w1,data=0.5914405264235476, grad=0),\n",
      "    Value(name=v038,layernumber=L3,neuronnumber=N1,weightnumber=,type=w2,data=-0.3725442040076463, grad=0),\n",
      "    Value(name=v039,layernumber=L3,neuronnumber=N1,weightnumber=,type=w3,data=0.3810827422406471, grad=0),\n",
      "    Value(name=v040,layernumber=L3,neuronnumber=N1,weightnumber=,type=w4,data=0.8301999957053683, grad=0),\n",
      "    Value(name=v041,layernumber=L3,neuronnumber=N1,weightnumber=,type=b,data=0, grad=0)]\n"
     ]
    }
   ],
   "source": [
    "# initialize a model \n",
    "model = MLP(3, [4, 4, 1]) # 2-layer neural network\n",
    "# print(model)\n",
    "# print(\"number of parameters\", len(model.parameters()))\n",
    "pp.pprint(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half Moon Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23716815 -0.17338508]\n",
      " [-0.55222318  0.57003313]\n",
      " [ 1.14252914 -0.02794639]\n",
      " [-1.05596279  0.11863834]\n",
      " [ 0.16985189  0.33087798]\n",
      " [-0.06995228  1.05829628]\n",
      " [ 0.80492904  0.58536957]\n",
      " [ 1.86706046  0.49985453]\n",
      " [ 1.57564151 -0.24506796]\n",
      " [ 1.12652106 -0.48793323]]\n",
      "[1 0 0 0 1 0 0 1 1 1]\n",
      "Module nn MLP: structure [2, 16, 16, 1]\n",
      "MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\n",
      "number of parameters 337\n",
      "Value(name=v7539,layernumber=,neuronnumber=,weightnumber=,type=,data=1.8456615030268986, grad=0) 0.3\n"
     ]
    }
   ],
   "source": [
    "# make up a dataset\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "number_of_samples=10\n",
    "number_of_iterations=10\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples=number_of_samples, noise=0.1)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "\n",
    "# initialize a model \n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))\n",
    "\n",
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1.8456615030268986, accuracy 30.0%\n",
      "step 1 loss 0.9025888803542371, accuracy 50.0%\n",
      "step 2 loss 0.9527683465162705, accuracy 50.0%\n",
      "step 3 loss 0.7041659714612418, accuracy 70.0%\n",
      "step 4 loss 0.7379767456259533, accuracy 60.0%\n",
      "step 5 loss 0.581212108075825, accuracy 70.0%\n",
      "step 6 loss 0.47789494591370996, accuracy 80.0%\n",
      "step 7 loss 0.45471480401530767, accuracy 80.0%\n",
      "step 8 loss 0.5256309335197832, accuracy 70.0%\n",
      "step 9 loss 0.49265313455711335, accuracy 80.0%\n"
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "for k in range(number_of_iterations):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "    \n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/number_of_iterations\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optimization manual steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from mycrograd_debug.engine_debug import Value\n",
    "from mycrograd_debug.nn_debug import MLP\n",
    "from mycrograd_debug.drawviz_debug import (\n",
    "    draw_dot,\n",
    "    draw_nn,\n",
    "    print_all_values,\n",
    "    print_my_params,\n",
    ")\n",
    "\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "number_of_iterations = 10\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "global activation\n",
    "global loss\n",
    "makeimg = False\n",
    "debug_parameters = True\n",
    "debug_values = False\n",
    "debug_steps=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron 1-2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module nn MLP: structure [1, 2, 1]\n",
      "parameters\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.50   0.00\n",
      " v002  L1  N1  b   0.00   0.00\n",
      " v003  L1  N2 w1   0.50   0.00\n",
      " v004  L1  N2  b   0.00   0.00\n",
      " v005  L2  N1 w1   0.60   0.00\n",
      " v006  L2  N1 w2   1.20   0.00\n",
      " v007  L2  N1  b   0.00   0.00\n",
      "inputs=  [Value(name=v008,layernumber=,neuronnumber=,weightnumber=,type=i,data=4, grad=0)]\n",
      "targets=  Value(name=v009,layernumber=,neuronnumber=,weightnumber=,type=t,data=1.2, grad=0)\n"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "nin = 1  # number of inputs\n",
    "nout = 1  # number of outputs\n",
    "Value.value_counter = 0\n",
    "\n",
    "model = MLP(nin, [2, nout], weightsinit=2, lastReLU=False, debug_bw=False)\n",
    "print(\"parameters\")\n",
    "print_my_params(model)\n",
    "xinumbers = list(range(4, 4 + nin))\n",
    "xinput = [Value(x, type=\"i\") for x in xinumbers]\n",
    "print(\"inputs= \", xinput)\n",
    "xtarget = Value(1.2, type=\"t\")  # desired targets\n",
    "print(\"targets= \", xtarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function single MLP\n",
    "def loss_single(activation, target):\n",
    "    total_loss = (activation - target)*(activation - target)\n",
    "    total_loss.type=\"l\"\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImage(filename):\n",
    "    # print(filename)\n",
    "    dot1=draw_dot(activation)\n",
    "    dot1.render(\"images/\"+filename , format=\"svg\", view=True)\n",
    "    dot2=draw_nn(xinput, model)\n",
    "    dot2.render(\"images/nn/\"+filename, format=\"svg\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act():\n",
    "    #### forward pass\n",
    "    global activation\n",
    "    global loss\n",
    "    activation = model(xinput)\n",
    "    # total_loss = activation\n",
    "    loss = loss_single(activation, xtarget)\n",
    "\n",
    "    if debug_parameters:\n",
    "        print_my_params(model)\n",
    "    if debug_values:\n",
    "        print_all_values(activation)\n",
    "\n",
    "\n",
    "def zeroGrad():\n",
    "    global activation\n",
    "    model.zero_grad()\n",
    "    for i in xinput:\n",
    "        i.grad = 0\n",
    "    if debug_parameters:\n",
    "        print_my_params(model)\n",
    "    if debug_values:\n",
    "        print_all_values(activation)\n",
    "\n",
    "\n",
    "def back():\n",
    "    #### backward pass\n",
    "    global activation\n",
    "    activation.backward()\n",
    "    if debug_parameters:\n",
    "        print_my_params(model)\n",
    "    if debug_values:\n",
    "        print_all_values(activation)\n",
    "\n",
    "\n",
    "def upd():\n",
    "    #### update\n",
    "    global activation\n",
    "    for p in model.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "    if debug_parameters:\n",
    "        print_my_params(model)\n",
    "    if debug_values:\n",
    "        print_all_values(activation)\n",
    "\n",
    "\n",
    "def opt():\n",
    "    global loss\n",
    "\n",
    "    for k in range(number_of_iterations):\n",
    "        print(\"start step %d\" % k)\n",
    "\n",
    "        if debug_steps:\n",
    "            print(\"step %d loss calc\" % k)\n",
    "        act()\n",
    "        if makeimg:\n",
    "            makeImage(\"opt_01_step%d_1loss\" % k)\n",
    "\n",
    "        if debug_steps:\n",
    "            print(\"step %d zero grad\" % k)\n",
    "        zeroGrad()\n",
    "        if makeimg:\n",
    "            makeImage(\"opt_01_step%d_2zero\" % k)\n",
    "\n",
    "        if debug_steps:\n",
    "            print(\"step %d backward\" % k)\n",
    "        back()\n",
    "        if makeimg:\n",
    "            makeImage(\"opt_01_step%d_3back\" % k)\n",
    "\n",
    "        if debug_steps:\n",
    "            print(\"step %d update\" % k)\n",
    "        upd()\n",
    "        if makeimg:\n",
    "            makeImage(\"opt_01_step%d_3upda\" % k)\n",
    "\n",
    "        print(f\"step %3d output %6.4f loss %6.4f\" % (k, activation.data, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start step 0\n",
      "step 0 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.50   0.00\n",
      " v002  L1  N1  b   0.00   0.00\n",
      " v003  L1  N2 w1   0.50   0.00\n",
      " v004  L1  N2  b   0.00   0.00\n",
      " v005  L2  N1 w1   0.60   0.00\n",
      " v006  L2  N1 w2   1.20   0.00\n",
      " v007  L2  N1  b   0.00   0.00\n",
      "step 0 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.50   0.00\n",
      " v002  L1  N1  b   0.00   0.00\n",
      " v003  L1  N2 w1   0.50   0.00\n",
      " v004  L1  N2  b   0.00   0.00\n",
      " v005  L2  N1 w1   0.60   0.00\n",
      " v006  L2  N1 w2   1.20   0.00\n",
      " v007  L2  N1  b   0.00   0.00\n",
      "step 0 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.50   2.40\n",
      " v002  L1  N1  b   0.00   0.60\n",
      " v003  L1  N2 w1   0.50   4.80\n",
      " v004  L1  N2  b   0.00   1.20\n",
      " v005  L2  N1 w1   0.60   2.00\n",
      " v006  L2  N1 w2   1.20   2.00\n",
      " v007  L2  N1  b   0.00   1.00\n",
      "step 0 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.38   2.40\n",
      " v002  L1  N1  b  -0.03   0.60\n",
      " v003  L1  N2 w1   0.26   4.80\n",
      " v004  L1  N2  b  -0.06   1.20\n",
      " v005  L2  N1 w1   0.50   2.00\n",
      " v006  L2  N1 w2   1.10   2.00\n",
      " v007  L2  N1  b  -0.05   1.00\n",
      "step   0 output 3.6000 loss 5.7600\n",
      "start step 1\n",
      "step 1 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.38   2.40\n",
      " v002  L1  N1  b  -0.03   0.60\n",
      " v003  L1  N2 w1   0.26   4.80\n",
      " v004  L1  N2  b  -0.06   1.20\n",
      " v005  L2  N1 w1   0.50   2.00\n",
      " v006  L2  N1 w2   1.10   2.00\n",
      " v007  L2  N1  b  -0.05   1.00\n",
      "step 1 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.38   0.00\n",
      " v002  L1  N1  b  -0.03   0.00\n",
      " v003  L1  N2 w1   0.26   0.00\n",
      " v004  L1  N2  b  -0.06   0.00\n",
      " v005  L2  N1 w1   0.50   0.00\n",
      " v006  L2  N1 w2   1.10   0.00\n",
      " v007  L2  N1  b  -0.05   0.00\n",
      "step 1 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.38   2.00\n",
      " v002  L1  N1  b  -0.03   0.50\n",
      " v003  L1  N2 w1   0.26   4.40\n",
      " v004  L1  N2  b  -0.06   1.10\n",
      " v005  L2  N1 w1   0.50   1.49\n",
      " v006  L2  N1 w2   1.10   0.98\n",
      " v007  L2  N1  b  -0.05   1.00\n",
      "step 1 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.28   2.00\n",
      " v002  L1  N1  b  -0.06   0.50\n",
      " v003  L1  N2 w1   0.04   4.40\n",
      " v004  L1  N2  b  -0.12   1.10\n",
      " v005  L2  N1 w1   0.43   1.49\n",
      " v006  L2  N1 w2   1.05   0.98\n",
      " v007  L2  N1  b  -0.10   1.00\n",
      "step   1 output 1.7730 loss 0.3283\n",
      "start step 2\n",
      "step 2 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.28   2.00\n",
      " v002  L1  N1  b  -0.06   0.50\n",
      " v003  L1  N2 w1   0.04   4.40\n",
      " v004  L1  N2  b  -0.12   1.10\n",
      " v005  L2  N1 w1   0.43   1.49\n",
      " v006  L2  N1 w2   1.05   0.98\n",
      " v007  L2  N1  b  -0.10   1.00\n",
      "step 2 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.28   0.00\n",
      " v002  L1  N1  b  -0.06   0.00\n",
      " v003  L1  N2 w1   0.04   0.00\n",
      " v004  L1  N2  b  -0.12   0.00\n",
      " v005  L2  N1 w1   0.43   0.00\n",
      " v006  L2  N1 w2   1.05   0.00\n",
      " v007  L2  N1  b  -0.10   0.00\n",
      "step 2 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.28   1.70\n",
      " v002  L1  N1  b  -0.06   0.43\n",
      " v003  L1  N2 w1   0.04   4.20\n",
      " v004  L1  N2  b  -0.12   1.05\n",
      " v005  L2  N1 w1   0.43   1.06\n",
      " v006  L2  N1 w2   1.05   0.04\n",
      " v007  L2  N1  b  -0.10   1.00\n",
      "step 2 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.19   1.70\n",
      " v002  L1  N1  b  -0.08   0.43\n",
      " v003  L1  N2 w1  -0.17   4.20\n",
      " v004  L1  N2  b  -0.17   1.05\n",
      " v005  L2  N1 w1   0.37   1.06\n",
      " v006  L2  N1 w2   1.05   0.04\n",
      " v007  L2  N1  b  -0.15   1.00\n",
      "step   2 output 0.4005 loss 0.6393\n",
      "start step 3\n",
      "step 3 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.19   1.70\n",
      " v002  L1  N1  b  -0.08   0.43\n",
      " v003  L1  N2 w1  -0.17   4.20\n",
      " v004  L1  N2  b  -0.17   1.05\n",
      " v005  L2  N1 w1   0.37   1.06\n",
      " v006  L2  N1 w2   1.05   0.04\n",
      " v007  L2  N1  b  -0.15   1.00\n",
      "step 3 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.19   0.00\n",
      " v002  L1  N1  b  -0.08   0.00\n",
      " v003  L1  N2 w1  -0.17   0.00\n",
      " v004  L1  N2  b  -0.17   0.00\n",
      " v005  L2  N1 w1   0.37   0.00\n",
      " v006  L2  N1 w2   1.05   0.00\n",
      " v007  L2  N1  b  -0.15   0.00\n",
      "step 3 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.19   1.49\n",
      " v002  L1  N1  b  -0.08   0.37\n",
      " v003  L1  N2 w1  -0.17   4.20\n",
      " v004  L1  N2  b  -0.17   1.05\n",
      " v005  L2  N1 w1   0.37   0.70\n",
      " v006  L2  N1 w2   1.05  -0.85\n",
      " v007  L2  N1  b  -0.15   1.00\n",
      "step 3 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.12   1.49\n",
      " v002  L1  N1  b  -0.09   0.37\n",
      " v003  L1  N2 w1  -0.38   4.20\n",
      " v004  L1  N2  b  -0.22   1.05\n",
      " v005  L2  N1 w1   0.34   0.70\n",
      " v006  L2  N1 w2   1.09  -0.85\n",
      " v007  L2  N1  b  -0.20   1.00\n",
      "step   3 output -0.7779 loss 3.9121\n",
      "start step 4\n",
      "step 4 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.12   1.49\n",
      " v002  L1  N1  b  -0.09   0.37\n",
      " v003  L1  N2 w1  -0.38   4.20\n",
      " v004  L1  N2  b  -0.22   1.05\n",
      " v005  L2  N1 w1   0.34   0.70\n",
      " v006  L2  N1 w2   1.09  -0.85\n",
      " v007  L2  N1  b  -0.20   1.00\n",
      "step 4 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.12   0.00\n",
      " v002  L1  N1  b  -0.09   0.00\n",
      " v003  L1  N2 w1  -0.38   0.00\n",
      " v004  L1  N2  b  -0.22   0.00\n",
      " v005  L2  N1 w1   0.34   0.00\n",
      " v006  L2  N1 w2   1.09   0.00\n",
      " v007  L2  N1  b  -0.20   0.00\n",
      "step 4 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.12   1.35\n",
      " v002  L1  N1  b  -0.09   0.34\n",
      " v003  L1  N2 w1  -0.38   4.36\n",
      " v004  L1  N2  b  -0.22   1.09\n",
      " v005  L2  N1 w1   0.34   0.39\n",
      " v006  L2  N1 w2   1.09  -1.74\n",
      " v007  L2  N1  b  -0.20   1.00\n",
      "step 4 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.05   1.35\n",
      " v002  L1  N1  b  -0.11   0.34\n",
      " v003  L1  N2 w1  -0.60   4.36\n",
      " v004  L1  N2  b  -0.27   1.09\n",
      " v005  L2  N1 w1   0.32   0.39\n",
      " v006  L2  N1 w2   1.18  -1.74\n",
      " v007  L2  N1  b  -0.25   1.00\n",
      "step   4 output -1.9680 loss 10.0361\n",
      "start step 5\n",
      "step 5 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.05   1.35\n",
      " v002  L1  N1  b  -0.11   0.34\n",
      " v003  L1  N2 w1  -0.60   4.36\n",
      " v004  L1  N2  b  -0.27   1.09\n",
      " v005  L2  N1 w1   0.32   0.39\n",
      " v006  L2  N1 w2   1.18  -1.74\n",
      " v007  L2  N1  b  -0.25   1.00\n",
      "step 5 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.05   0.00\n",
      " v002  L1  N1  b  -0.11   0.00\n",
      " v003  L1  N2 w1  -0.60   0.00\n",
      " v004  L1  N2  b  -0.27   0.00\n",
      " v005  L2  N1 w1   0.32   0.00\n",
      " v006  L2  N1 w2   1.18   0.00\n",
      " v007  L2  N1  b  -0.25   0.00\n",
      "step 5 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1   0.05   1.27\n",
      " v002  L1  N1  b  -0.11   0.32\n",
      " v003  L1  N2 w1  -0.60   4.71\n",
      " v004  L1  N2  b  -0.27   1.18\n",
      " v005  L2  N1 w1   0.32   0.10\n",
      " v006  L2  N1 w2   1.18  -2.67\n",
      " v007  L2  N1  b  -0.25   1.00\n",
      "step 5 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.01   1.27\n",
      " v002  L1  N1  b  -0.13   0.32\n",
      " v003  L1  N2 w1  -0.83   4.71\n",
      " v004  L1  N2  b  -0.33   1.18\n",
      " v005  L2  N1 w1   0.31   0.10\n",
      " v006  L2  N1 w2   1.31  -2.67\n",
      " v007  L2  N1  b  -0.30   1.00\n",
      "step   5 output -3.3606 loss 20.7989\n",
      "start step 6\n",
      "step 6 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.01   1.27\n",
      " v002  L1  N1  b  -0.13   0.32\n",
      " v003  L1  N2 w1  -0.83   4.71\n",
      " v004  L1  N2  b  -0.33   1.18\n",
      " v005  L2  N1 w1   0.31   0.10\n",
      " v006  L2  N1 w2   1.31  -2.67\n",
      " v007  L2  N1  b  -0.30   1.00\n",
      "step 6 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.01   0.00\n",
      " v002  L1  N1  b  -0.13   0.00\n",
      " v003  L1  N2 w1  -0.83   0.00\n",
      " v004  L1  N2  b  -0.33   0.00\n",
      " v005  L2  N1 w1   0.31   0.00\n",
      " v006  L2  N1 w2   1.31   0.00\n",
      " v007  L2  N1  b  -0.30   0.00\n",
      "step 6 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.01   1.25\n",
      " v002  L1  N1  b  -0.13   0.31\n",
      " v003  L1  N2 w1  -0.83   5.25\n",
      " v004  L1  N2  b  -0.33   1.31\n",
      " v005  L2  N1 w1   0.31  -0.17\n",
      " v006  L2  N1 w2   1.31  -3.67\n",
      " v007  L2  N1  b  -0.30   1.00\n",
      "step 6 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.07   1.25\n",
      " v002  L1  N1  b  -0.14   0.31\n",
      " v003  L1  N2 w1  -1.10   5.25\n",
      " v004  L1  N2  b  -0.40   1.31\n",
      " v005  L2  N1 w1   0.32  -0.17\n",
      " v006  L2  N1 w2   1.49  -3.67\n",
      " v007  L2  N1  b  -0.35   1.00\n",
      "step   6 output -5.1647 loss 40.5089\n",
      "start step 7\n",
      "step 7 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.07   1.25\n",
      " v002  L1  N1  b  -0.14   0.31\n",
      " v003  L1  N2 w1  -1.10   5.25\n",
      " v004  L1  N2  b  -0.40   1.31\n",
      " v005  L2  N1 w1   0.32  -0.17\n",
      " v006  L2  N1 w2   1.49  -3.67\n",
      " v007  L2  N1  b  -0.35   1.00\n",
      "step 7 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.07   0.00\n",
      " v002  L1  N1  b  -0.14   0.00\n",
      " v003  L1  N2 w1  -1.10   0.00\n",
      " v004  L1  N2  b  -0.40   0.00\n",
      " v005  L2  N1 w1   0.32   0.00\n",
      " v006  L2  N1 w2   1.49   0.00\n",
      " v007  L2  N1  b  -0.35   0.00\n",
      "step 7 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.07   1.28\n",
      " v002  L1  N1  b  -0.14   0.32\n",
      " v003  L1  N2 w1  -1.10   5.98\n",
      " v004  L1  N2  b  -0.40   1.49\n",
      " v005  L2  N1 w1   0.32  -0.44\n",
      " v006  L2  N1 w2   1.49  -4.78\n",
      " v007  L2  N1  b  -0.35   1.00\n",
      "step 7 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.14   1.28\n",
      " v002  L1  N1  b  -0.16   0.32\n",
      " v003  L1  N2 w1  -1.40   5.98\n",
      " v004  L1  N2  b  -0.47   1.49\n",
      " v005  L2  N1 w1   0.34  -0.44\n",
      " v006  L2  N1 w2   1.73  -4.78\n",
      " v007  L2  N1  b  -0.40   1.00\n",
      "step   7 output -7.6410 loss 78.1636\n",
      "start step 8\n",
      "step 8 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.14   1.28\n",
      " v002  L1  N1  b  -0.16   0.32\n",
      " v003  L1  N2 w1  -1.40   5.98\n",
      " v004  L1  N2  b  -0.47   1.49\n",
      " v005  L2  N1 w1   0.34  -0.44\n",
      " v006  L2  N1 w2   1.73  -4.78\n",
      " v007  L2  N1  b  -0.40   1.00\n",
      "step 8 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.14   0.00\n",
      " v002  L1  N1  b  -0.16   0.00\n",
      " v003  L1  N2 w1  -1.40   0.00\n",
      " v004  L1  N2  b  -0.47   0.00\n",
      " v005  L2  N1 w1   0.34   0.00\n",
      " v006  L2  N1 w2   1.73   0.00\n",
      " v007  L2  N1  b  -0.40   0.00\n",
      "step 8 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.14   1.37\n",
      " v002  L1  N1  b  -0.16   0.34\n",
      " v003  L1  N2 w1  -1.40   6.94\n",
      " v004  L1  N2  b  -0.47   1.73\n",
      " v005  L2  N1 w1   0.34  -0.71\n",
      " v006  L2  N1 w2   1.73  -6.05\n",
      " v007  L2  N1  b  -0.40   1.00\n",
      "step 8 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.21   1.37\n",
      " v002  L1  N1  b  -0.18   0.34\n",
      " v003  L1  N2 w1  -1.74   6.94\n",
      " v004  L1  N2  b  -0.56   1.73\n",
      " v005  L2  N1 w1   0.38  -0.71\n",
      " v006  L2  N1 w2   2.04  -6.05\n",
      " v007  L2  N1  b  -0.45   1.00\n",
      "step   8 output -11.1418 loss 152.3205\n",
      "start step 9\n",
      "step 9 loss calc\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.21   1.37\n",
      " v002  L1  N1  b  -0.18   0.34\n",
      " v003  L1  N2 w1  -1.74   6.94\n",
      " v004  L1  N2  b  -0.56   1.73\n",
      " v005  L2  N1 w1   0.38  -0.71\n",
      " v006  L2  N1 w2   2.04  -6.05\n",
      " v007  L2  N1  b  -0.45   1.00\n",
      "step 9 zero grad\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.21   0.00\n",
      " v002  L1  N1  b  -0.18   0.00\n",
      " v003  L1  N2 w1  -1.74   0.00\n",
      " v004  L1  N2  b  -0.56   0.00\n",
      " v005  L2  N1 w1   0.38   0.00\n",
      " v006  L2  N1 w2   2.04   0.00\n",
      " v007  L2  N1  b  -0.45   0.00\n",
      "step 9 backward\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.21   1.51\n",
      " v002  L1  N1  b  -0.18   0.38\n",
      " v003  L1  N2 w1  -1.74   8.15\n",
      " v004  L1  N2  b  -0.56   2.04\n",
      " v005  L2  N1 w1   0.38  -1.00\n",
      " v006  L2  N1 w2   2.04  -7.53\n",
      " v007  L2  N1  b  -0.45   1.00\n",
      "step 9 update\n",
      " name lay neu ty   data   grad\n",
      " v001  L1  N1 w1  -0.28   1.51\n",
      " v002  L1  N1  b  -0.20   0.38\n",
      " v003  L1  N2 w1  -2.15   8.15\n",
      " v004  L1  N2  b  -0.66   2.04\n",
      " v005  L2  N1 w1   0.43  -1.00\n",
      " v006  L2  N1 w2   2.41  -7.53\n",
      " v007  L2  N1  b  -0.50   1.00\n",
      "step   9 output -16.1622 loss 301.4476\n"
     ]
    }
   ],
   "source": [
    "opt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
